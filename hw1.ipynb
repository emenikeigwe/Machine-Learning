{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23641c0c-e14f-42cd-9b96-b4acc50196d2",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Assignment #1: KNN Classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "feda907b-2181-4fb0-afde-37759fab59f8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ucimlrepo in /Users/mnek/anaconda3/lib/python3.11/site-packages (0.0.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install ucimlrepo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "ed6ee80f-b0f8-4224-864f-29cbf17942aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "from ucimlrepo import fetch_ucirepo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9297562b-2551-4c74-8ea5-33519f166474",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1. Write a function to calculate and return the Minkowski distance with optional argument p defaulting to ‘p=2’ (Euclidean) of two vectors where a vector represents a data point."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f0ce3d-92bf-4d81-9fd8-2dc2ae6a7498",
   "metadata": {},
   "source": [
    "Note: Minkowski Distance is the generalized form of distance calculations with p=1 representing Manhattan distance and p=2 representing Euclidean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "ee1b348e-bacb-4458-9b67-7bd61ed9721c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def minkowski_dist(v1: np.ndarray, v2: np.ndarray, p: int=2):\n",
    "    return np.sum((np.absolute(np.power(np.subtract(v1, v2), p))))**(1.0/p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "b58d895a-51cc-4f92-9f2d-1bedc152071b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# minkowski_dist([1,3,5,7],[2,4,6,8], p=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "055e532d-862a-41d9-8da7-c2cc96710c31",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# minkowski_dist([1,3,5,7],[2,4,6,8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "2b23c10c-17cc-44ce-a17e-e4639a0e8d1c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# minkowski_dist([1,3,5,7],[2,4,6,8], p=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "4dd7d0ae-a2ad-42a5-bb1b-753074b767e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# minkowski_dist([1,3,5,7],[2,4,6,8], p=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb53c47d-670e-4199-befd-46e8f5b49665",
   "metadata": {},
   "source": [
    "### 2. Write a function to calculate and return the accuracy of two vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "4aa27b48-dad6-4e30-9ba9-d3141714cc38",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def accuracy(v1: np.ndarray, v2: np.ndarray):\n",
    "    assert len(v1) == len(v2)\n",
    "    return np.sum(v1 == v2)/np.size(v1)\n",
    "    # return sum([X[i] == Y[i] for i in range(len(X))])/len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "47e6ead6-3c3e-48b0-8c7b-4ad89a7c5d24",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# X = np.asarray(list(range(20)))\n",
    "# Y = np.asarray(list(range(20)))\n",
    "# Y[3], Y[10], X[6] = 10, 3, 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "1334f01c-98cc-4080-8f67-c66f1210b75e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# accuracy(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df4fd4f-43d3-49e4-ace7-bffe9b90517f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3. Write three functions to compute: precision, recall and F1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "75113c9c-da9f-409f-8c4d-87f1c9f9338e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def precision(v1: np.ndarray, v2: np.ndarray):\n",
    "    # true positive over predicted positive\n",
    "    # precision measures how accurate your positive predictions are\n",
    "    # which percentage of your positive predictions are correct\n",
    "    # !! How many retrieved items are relevant?\n",
    "    # true positives out of retrieved\n",
    "    true_pos = 0\n",
    "    false_pos = 0\n",
    "    for i in range(len(v1)):\n",
    "        if v1[i] and v2[i]:\n",
    "            true_pos += 1\n",
    "        if not v1[i] and v2[i]:\n",
    "            false_pos += 1\n",
    "\n",
    "    return 1.0 * true_pos / (true_pos + false_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "783a8ebf-9aeb-4ec3-bfa3-e534dfebf27f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def recall(v1: np.ndarray, v2: np.ndarray):\n",
    "    # true positive over real positive\n",
    "    # recall measures how well you find all the actual positives\n",
    "    # which percentage of actual positive samples were correctly classified\n",
    "    # !! How many relevant items are retrieved?\n",
    "    # true positives out of all actual positives\n",
    "    true_pos = 0\n",
    "    false_neg = 0\n",
    "    \n",
    "    for i in range(len(v1)):\n",
    "        if v1[i] and v2[i]:\n",
    "            true_pos += 1\n",
    "        if v1[i] and not v2[i]:\n",
    "            false_neg += 1\n",
    "    return true_pos/(true_pos + false_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "2609ef55-837c-4abc-9dfc-283c990a9bfa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def F1(v1: np.ndarray, v2: np.ndarray):\n",
    "    pre = precision(v1, v2)\n",
    "    rec = recall(v1, v2)\n",
    "    \n",
    "    return 2*(pre*rec)/(pre + rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "f37b6db2-46b8-442c-a58a-34bf96de5976",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# X = np.asarray([0, 0, 1, 1, 0, 0, 0, 1])\n",
    "# Y = np.asarray([0, 0, 1, 0, 1, 0, 1, 0])\n",
    "# print(accuracy(X, Y))\n",
    "# print(precision(X, Y))\n",
    "# print(recall(X, Y))\n",
    "# print(F1(X, Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9287b51d-5a82-421a-b186-a62cbcfeafa8",
   "metadata": {},
   "source": [
    "### 4. Write a function to compute the confusion matrix of two vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "0d0bb2dc-bd75-4ace-b623-afdb750aa163",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def confusion_matrix(X: np.ndarray, Y: np.ndarray):\n",
    "    true_neg = 0\n",
    "    false_pos = 0\n",
    "    false_neg = 0\n",
    "    true_pos = 0\n",
    "    \n",
    "    for i in range(len(X)):\n",
    "        if X[i] == Y[i] and not X[i]:\n",
    "            true_neg += 1\n",
    "        if X[i] != Y[i] and not X[i]:\n",
    "            false_pos += 1\n",
    "        if X[i] != Y[i] and X[i]:\n",
    "            false_neg += 1\n",
    "        if X[i] == Y[i] and X[i]:\n",
    "            true_pos += 1\n",
    "    return [[true_neg, false_pos], [false_neg, true_pos]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "83d36842-f6a2-4d9c-af62-b2c7d878c733",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print(confusion_matrix(X, Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43ac858-84dc-4a8b-83a4-46beebd4dfdb",
   "metadata": {},
   "source": [
    "### 5. Write a function to generate the Receiver Operating Characteristic (ROC) curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "a1b7c4d6-0056-4f32-8fe0-14be1832e1cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# need actual data\n",
    "# need repr matching threshold % w/ predictions from that threshold (or FPR & TPR at least)\n",
    "def predict_from_threshold(actual_data, model):\n",
    "    pass\n",
    "\n",
    "def roc(fpr_by_threshold: dict, tpr_by_threshold: dict):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad79c80-e870-4d0e-a784-2081026cc0ea",
   "metadata": {},
   "source": [
    "### 6. Write a function to compute area under curve (AUC) for the ROC curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "2780cf6c-064d-411d-8c6d-d193969e4929",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def auc():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0806202-b81b-42ab-b8cc-a20f4bbf1909",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 7. Write a function to generate the precision-recall curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "77b37ac7-0552-49c8-aaf7-c30201de992c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def precision_recall():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab02a7a-ea64-49b8-b9dc-9100b4dcfa18",
   "metadata": {},
   "source": [
    "## 8. Implement a KNN_Classifier model class. It should have the following three methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1892bb0-0566-4dcd-89de-340213f98774",
   "metadata": {},
   "source": [
    "#### a) __init__(self,) It’s a standard python initialization function so we can instantiate the class. Just “pass” this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb4729a-217a-4b4f-af73-8ad834c69fb0",
   "metadata": {},
   "source": [
    "#### b) fit(self, X, Y) This method simply needs to store the relevant values as instance variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6b4e73-207f-4c40-8cbf-fbf07616c257",
   "metadata": {},
   "source": [
    "#### c) predict(self, X,threshold=.5) This method will use the instance variables stored by the fit method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "aef2e600-9701-4f38-ad58-edb650b0f00b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class KNN_Classifier:\n",
    "    def __init__(self, n_neighbors: int, weights: str=\"uniform\", p: int=2) -> None:\n",
    "        self.n_neighbors = n_neighbors\n",
    "        self.weights = weights\n",
    "        self.p = p\n",
    "        self.X_ = None\n",
    "        self.Y_ = None\n",
    "\n",
    "        \n",
    "    def fit(self, X, Y) -> None:\n",
    "        self.X_ = X\n",
    "        self.Y_ = Y\n",
    "    \n",
    "    def predict(self, X: np.ndarray, threshold: float=.5) -> np.ndarray:\n",
    "        probabilities = self.predict_proba(X)\n",
    "        predictions = []\n",
    "        for prob in probabilities:\n",
    "            if prob >= threshold:\n",
    "                predictions.append([1])\n",
    "            else:\n",
    "                predictions.append([0])\n",
    "        return np.asarray(predictions)\n",
    "    \n",
    "    def predict_proba(self, X: np.ndarray) -> np.ndarray:\n",
    "        probabilities = []\n",
    "        # looping through every x we want to predict\n",
    "        for x in X:\n",
    "            distances = []\n",
    "            # looping through training data rows\n",
    "            for i in range(len(self.X_)):\n",
    "                # find distance\n",
    "                distance = minkowski_dist(x, self.X_[i], self.p)\n",
    "                # weight points based on weight metric; adj made for dist of zero\n",
    "                factor = 1\n",
    "                if self.weights == \"distance\":\n",
    "                    factor = 1.0/(distance + 1)\n",
    "                print(distance)\n",
    "                distance *= factor\n",
    "                print(distance)\n",
    "                print()\n",
    "                # factor = 1 if self.weights == \"uniform\" else 1.0/(distance + 1)\n",
    "                # add tuple of x, y, and distance\n",
    "                distances.append((self.X_[i], self.Y_[i], distance))\n",
    "            # find the k nearest neighbors\n",
    "            neighbors = sorted(distances, key=lambda tup: tup[2])[:self.n_neighbors]\n",
    "            # calculate and store positive class probability\n",
    "            probabilities.append(sum([n[1] for n in neighbors])/self.n_neighbors)\n",
    "        return np.asarray(probabilities)\n",
    "    \n",
    "    def get_params(self):\n",
    "        return {\"n_neighbors\": self.n_neighbors, \"weights\": self.weights, \"p\": self.p}\n",
    "    \n",
    "    def set_params(self, **params: dict) -> None:\n",
    "        self.n_neighbors = params.get(\"n_neighbors\", self.n_neighbors)\n",
    "        self.weights = params.get(\"weights\", self.weights)\n",
    "        self.p = params.get(\"p\", self.p)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2e7585",
   "metadata": {},
   "source": [
    "### 9. Write a function named “partition” to split your data into training and test sets. The function should take 4 arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "b2f9ea49-265c-49b9-9816-a40cb671d2e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def partition(feature: np.ndarray, target: np.ndarray, t: float, shuffle: bool=True) -> tuple:\n",
    "    training_size = int(t*len(feature))\n",
    "    samples = [(feature[i], target[i]) for i in range(len(feature))]\n",
    "    if shuffle:\n",
    "        p = np.random.permutation(len(feature))\n",
    "        feature = feature[p]\n",
    "        target = target[p]\n",
    "    return (feature[:training_size], feature[training_size:], target[:training_size], target[training_size:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207ca638",
   "metadata": {},
   "source": [
    "### 10. Read in the winequality-white.csv file as a Pandas data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "cc9d1e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch dataset \n",
    "wine_quality = fetch_ucirepo(id=186) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "features = wine_quality.data.features.copy()\n",
    "# features = features.drop([\"density\", \"pH\", \"chlorides\"], axis=1)\n",
    "targets = wine_quality.data.targets.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f2a8b0",
   "metadata": {},
   "source": [
    "### 11. The target will be the “quality” column which represents the rating of wine and ranges from 3 to 8. You will need to convert it into a two-category variable consisting of “good” (quality > 5) & “bad” (quality <= 5). Your target vector should have 0s (representing “bad” quality wine) and 1s (representing “good” quality wine)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "6bbf4aca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      fixed_acidity  volatile_acidity  citric_acid  residual_sugar  chlorides  \\\n",
      "0               7.4              0.70         0.00             1.9      0.076   \n",
      "1               7.8              0.88         0.00             2.6      0.098   \n",
      "2               7.8              0.76         0.04             2.3      0.092   \n",
      "3              11.2              0.28         0.56             1.9      0.075   \n",
      "4               7.4              0.70         0.00             1.9      0.076   \n",
      "...             ...               ...          ...             ...        ...   \n",
      "6492            6.2              0.21         0.29             1.6      0.039   \n",
      "6493            6.6              0.32         0.36             8.0      0.047   \n",
      "6494            6.5              0.24         0.19             1.2      0.041   \n",
      "6495            5.5              0.29         0.30             1.1      0.022   \n",
      "6496            6.0              0.21         0.38             0.8      0.020   \n",
      "\n",
      "      free_sulfur_dioxide  total_sulfur_dioxide  density    pH  sulphates  \\\n",
      "0                    11.0                  34.0  0.99780  3.51       0.56   \n",
      "1                    25.0                  67.0  0.99680  3.20       0.68   \n",
      "2                    15.0                  54.0  0.99700  3.26       0.65   \n",
      "3                    17.0                  60.0  0.99800  3.16       0.58   \n",
      "4                    11.0                  34.0  0.99780  3.51       0.56   \n",
      "...                   ...                   ...      ...   ...        ...   \n",
      "6492                 24.0                  92.0  0.99114  3.27       0.50   \n",
      "6493                 57.0                 168.0  0.99490  3.15       0.46   \n",
      "6494                 30.0                 111.0  0.99254  2.99       0.46   \n",
      "6495                 20.0                 110.0  0.98869  3.34       0.38   \n",
      "6496                 22.0                  98.0  0.98941  3.26       0.32   \n",
      "\n",
      "      alcohol  \n",
      "0         9.4  \n",
      "1         9.8  \n",
      "2         9.8  \n",
      "3         9.8  \n",
      "4         9.4  \n",
      "...       ...  \n",
      "6492     11.2  \n",
      "6493      9.6  \n",
      "6494      9.4  \n",
      "6495     12.8  \n",
      "6496     11.8  \n",
      "\n",
      "[6497 rows x 11 columns]\n",
      "      quality\n",
      "0           1\n",
      "1           1\n",
      "2           1\n",
      "3           1\n",
      "4           1\n",
      "...       ...\n",
      "6492        1\n",
      "6493        1\n",
      "6494        1\n",
      "6495        1\n",
      "6496        1\n",
      "\n",
      "[6497 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "# transform quality column into binary \"good\" and \"bad\"\n",
    "targets[\"quality\"] = np.where(targets[\"quality\"] < 5, 0, 1)\n",
    "\n",
    "\n",
    "features_np = features.to_numpy()\n",
    "targets_np = targets.to_numpy()\n",
    "\n",
    "print(features)\n",
    "print(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "055a6c50-a48d-4bc1-9987-b5d2906d12a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#classifier = KNN_Classifier(n_neighbors=5, weights=\"distance\", p=2)\n",
    "#classifier.fit(features_training, target_training)\n",
    "#classifier.get_params()\n",
    "#b = classifier.predict(features_test)\n",
    "# accuracies = []\n",
    "# classifier = KNN_Classifier(n_neighbors=5, weights=\"distance\", p=2)\n",
    "# for i in range(5):\n",
    "#     features_training, features_test, target_training, target_test = partition(features, targets, .8)\n",
    "#     classifier.fit(features_training, target_training)\n",
    "#     prediction = classifier.predict(features_test)\n",
    "#     accuracies.append(accuracy(prediction, target_test))\n",
    "# print(accuracies)\n",
    "# print(sum(accuracies)/len(accuracies))\n",
    "# classifier = KNN_Classifier(n_neighbors=5, weights=\"distance\", p=2)\n",
    "# classifier.fit(features, targets)\n",
    "# predictions = classifier.predict(features)\n",
    "# print(predictions)\n",
    "# print(accuracy(predictions, targets))\n",
    "\n",
    "# print(F1(prediction, targets))\n",
    "# print(recall(prediction, targets))\n",
    "# print(precision(prediction, targets))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8967e8-4cf4-4be3-a425-7f9baf5f812f",
   "metadata": {},
   "source": [
    "### 12. Provide a table with univariate statistics of your data (mean, standard deviation, and quartiles, min, max, missing count, number of unique values)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961b0982-3b3d-4f2e-a054-788225a3ea35",
   "metadata": {},
   "source": [
    "i'm unsure of how this should be represented? we could have rows as um the feature in question? and we could also have columns as the univariate statistic. we could make separate tables for each of the features and then construct an individual table with the statistics as single column names. it would likely be a 1x7 matrix excluding the table labels. actually now that i think about it it makes more sense for the rows of the table to be each individual statistical feature\n",
    "\n",
    "\n",
    "it could look something like this:\n",
    "\n",
    "   mean | std. dev | quartiles | min | max | missing count | number of unique values\n",
    "f1 \n",
    "f2\n",
    "f3\n",
    "f4\n",
    "f5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "665f2382-b2fc-483b-8cbd-a03ae083a107",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed_acidity</th>\n",
       "      <th>volatile_acidity</th>\n",
       "      <th>citric_acid</th>\n",
       "      <th>residual_sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free_sulfur_dioxide</th>\n",
       "      <th>total_sulfur_dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>6497.000000</td>\n",
       "      <td>6497.000000</td>\n",
       "      <td>6497.000000</td>\n",
       "      <td>6497.000000</td>\n",
       "      <td>6497.000000</td>\n",
       "      <td>6497.000000</td>\n",
       "      <td>6497.000000</td>\n",
       "      <td>6497.000000</td>\n",
       "      <td>6497.000000</td>\n",
       "      <td>6497.000000</td>\n",
       "      <td>6497.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>7.215307</td>\n",
       "      <td>0.339666</td>\n",
       "      <td>0.318633</td>\n",
       "      <td>5.443235</td>\n",
       "      <td>0.056034</td>\n",
       "      <td>30.525319</td>\n",
       "      <td>115.744574</td>\n",
       "      <td>0.994697</td>\n",
       "      <td>3.218501</td>\n",
       "      <td>0.531268</td>\n",
       "      <td>10.491801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.296434</td>\n",
       "      <td>0.164636</td>\n",
       "      <td>0.145318</td>\n",
       "      <td>4.757804</td>\n",
       "      <td>0.035034</td>\n",
       "      <td>17.749400</td>\n",
       "      <td>56.521855</td>\n",
       "      <td>0.002999</td>\n",
       "      <td>0.160787</td>\n",
       "      <td>0.148806</td>\n",
       "      <td>1.192712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>3.800000</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.009000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.987110</td>\n",
       "      <td>2.720000</td>\n",
       "      <td>0.220000</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>6.400000</td>\n",
       "      <td>0.230000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>1.800000</td>\n",
       "      <td>0.038000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>77.000000</td>\n",
       "      <td>0.992340</td>\n",
       "      <td>3.110000</td>\n",
       "      <td>0.430000</td>\n",
       "      <td>9.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>7.000000</td>\n",
       "      <td>0.290000</td>\n",
       "      <td>0.310000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.047000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>118.000000</td>\n",
       "      <td>0.994890</td>\n",
       "      <td>3.210000</td>\n",
       "      <td>0.510000</td>\n",
       "      <td>10.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>7.700000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.390000</td>\n",
       "      <td>8.100000</td>\n",
       "      <td>0.065000</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>156.000000</td>\n",
       "      <td>0.996990</td>\n",
       "      <td>3.320000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>11.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>15.900000</td>\n",
       "      <td>1.580000</td>\n",
       "      <td>1.660000</td>\n",
       "      <td>65.800000</td>\n",
       "      <td>0.611000</td>\n",
       "      <td>289.000000</td>\n",
       "      <td>440.000000</td>\n",
       "      <td>1.038980</td>\n",
       "      <td>4.010000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>14.900000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       fixed_acidity  volatile_acidity  citric_acid  residual_sugar  \\\n",
       "count    6497.000000       6497.000000  6497.000000     6497.000000   \n",
       "mean        7.215307          0.339666     0.318633        5.443235   \n",
       "std         1.296434          0.164636     0.145318        4.757804   \n",
       "min         3.800000          0.080000     0.000000        0.600000   \n",
       "25%         6.400000          0.230000     0.250000        1.800000   \n",
       "50%         7.000000          0.290000     0.310000        3.000000   \n",
       "75%         7.700000          0.400000     0.390000        8.100000   \n",
       "max        15.900000          1.580000     1.660000       65.800000   \n",
       "\n",
       "         chlorides  free_sulfur_dioxide  total_sulfur_dioxide      density  \\\n",
       "count  6497.000000          6497.000000           6497.000000  6497.000000   \n",
       "mean      0.056034            30.525319            115.744574     0.994697   \n",
       "std       0.035034            17.749400             56.521855     0.002999   \n",
       "min       0.009000             1.000000              6.000000     0.987110   \n",
       "25%       0.038000            17.000000             77.000000     0.992340   \n",
       "50%       0.047000            29.000000            118.000000     0.994890   \n",
       "75%       0.065000            41.000000            156.000000     0.996990   \n",
       "max       0.611000           289.000000            440.000000     1.038980   \n",
       "\n",
       "                pH    sulphates      alcohol  \n",
       "count  6497.000000  6497.000000  6497.000000  \n",
       "mean      3.218501     0.531268    10.491801  \n",
       "std       0.160787     0.148806     1.192712  \n",
       "min       2.720000     0.220000     8.000000  \n",
       "25%       3.110000     0.430000     9.500000  \n",
       "50%       3.210000     0.510000    10.300000  \n",
       "75%       3.320000     0.600000    11.300000  \n",
       "max       4.010000     2.000000    14.900000  "
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scipy.stats as sp\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "# .preprocessing.StandardScaler\n",
    "targets_description = targets.describe(include='all')\n",
    "# targets_description.loc['unique'] = len(np.unique(targets.dropna(), axis=0))\n",
    "# targets_description.loc['missing'] = None\n",
    "targets_description\n",
    "features_description = features.describe(include='all')\n",
    "# features_description.loc['unique'] = len(np.unique(features.dropna(), axis=0))\n",
    "# features_description.loc['missing'] = None\n",
    "features_description\n",
    "\n",
    "#targets.describe(include = 'all')\n",
    "#df[\"missing\"] = None\n",
    "#df[\"num unique\"] = np.asarray([len(x) for x in np.unique(features, axis=0)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470c9216-99d9-4913-8cf7-d5e68dd3d67a",
   "metadata": {},
   "source": [
    "### 13. Generate pair plots using the seaborn package to help identify redundant features. For any redundant features(?), report, drop, and explain your logic (w/ markdown). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29657ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f3421419-372c-4de1-83b9-e0b7b69000a0",
   "metadata": {},
   "source": [
    "### 14. Use your “partition” function to split the data into 80% train and 20% test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "d430abf5-faa5-4806-b07e-9efc6f3237df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "features_training, features_test, target_training, target_test = partition(features_np, targets_np, .8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be33023-a9ef-4093-bc8f-df965eb58d80",
   "metadata": {},
   "source": [
    "### 15. Naively run your KNN_Classifier model on the training dataset with n_neighbors = 5 and using Euclidean distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "4a2c632a-e6a2-4caf-8efa-8d6c9904e8df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "classifier = KNN_Classifier(n_neighbors=5, p=2)\n",
    "classifier.fit(features_training, target_training)\n",
    "nonstd_predictions = classifier.predict(features_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72172639-5e26-455c-8bdc-070d8721b3f3",
   "metadata": {},
   "source": [
    "#### a. Use accuracy and F1 score to compare your predictions to the expected labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "31da3d00-b2c4-4ac5-ace4-f4f54a247d39",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.9638461538461538\n",
      "F1: 0.981561396626128\n"
     ]
    }
   ],
   "source": [
    "print(\"accuracy:\", accuracy(nonstd_predictions, target_test))\n",
    "print(\"F1:\", F1(nonstd_predictions, target_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3331cf-c21d-4da9-9822-8b363c93b4da",
   "metadata": {},
   "source": [
    "##### b. Now standardize each feature of your training set (subtract mean and divide by standard deviation) and apply trained standardization to the test set. Use the mean and standard deviation values for each feature in the training set to scale the test data (you can use sklearn.preprocessing.StandardScaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "6b6b2841-e7ef-4842-8246-e855888d4080",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "# find mean and standard deviation of training set\n",
    "# use mean/std dev to standardize the training set\n",
    "# use that same mean and std. dev to standardize the test data\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "scaler.fit(features_training)\n",
    "features_training_std = scaler.transform(features_training)\n",
    "features_test_std = scaler.transform(features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "71ad030c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(scaler.mean_)\n",
    "# print(features_training[0])\n",
    "# print([a[0] for a in features_training_std])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "70704db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(np.mean(features_training_std, axis=1))\n",
    "# print(features_training_std[0])\n",
    "# m = sum(a[0] for a in features_training_std)/len(features_training_std)\n",
    "# print(m)\n",
    "# print((sum([(a[0]-m)**2 for a in features_training_std])/len(features_training_std))**.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "72265546",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Just checking my work'"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Just checking my work\"\"\"\n",
    "# m1 = sum([a[1] for a in features_training])/len(features_training)\n",
    "# st1 = (sum([(a[1]-m1)**2 for a in features_training])/len(features_training))**.5\n",
    "# print(m1)\n",
    "# print(st1)\n",
    "# print([a[1] for a in features_training])\n",
    "# print([(a[1]-m1)/st1 for a in features_training])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b72b77",
   "metadata": {},
   "source": [
    "##### c. Re-run the KNN_Classifier model on the standardized data, find the accuracy and F1 score with the expected labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "ea326640",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = KNN_Classifier(n_neighbors=5, p=2)\n",
    "classifier.fit(features_training_std, target_training)\n",
    "std_predictions = classifier.predict(features_test_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "235eb68e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy (standardized): 0.9646153846153847\n",
      "F1 (standardized): 0.9819324430479183\n"
     ]
    }
   ],
   "source": [
    "print(\"accuracy (standardized):\", accuracy(std_predictions, target_test))\n",
    "print(\"F1 (standardized):\", F1(std_predictions, target_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d045097",
   "metadata": {},
   "source": [
    "    Result: The model using non-standardized data performs slightly better than the standardized version. I will utilize the non-standardized data for the rest of the assignment even though standardized data (in theory) performs better than non-standardized data in most cases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4beb84e8",
   "metadata": {},
   "source": [
    "##### e. Perform a similar test for inverse distance weighting in the KNN_Classifier model and determine whether or not to use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920a4caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with IDW\n",
    "\n",
    "classifier = KNN_Classifier(n_neighbors=5, weights=\"distance\", p=2)\n",
    "classifier.fit(features_training, target_training)\n",
    "nonstd_predictions = classifier.predict(features_test)\n",
    "\n",
    "print(\"accuracy:\", accuracy(nonstd_predictions, target_test))\n",
    "print(\"F1:\", F1(nonstd_predictions, target_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "eb4f78b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.9676923076923077\n",
      "F1: 0.983580922595778\n"
     ]
    }
   ],
   "source": [
    "# without IDW\n",
    "\n",
    "classifier = KNN_Classifier(n_neighbors=5, p=2)\n",
    "classifier.fit(features_training, target_training)\n",
    "nonstd_predictions = classifier.predict(features_test)\n",
    "\n",
    "print(\"accuracy:\", accuracy(nonstd_predictions, target_test))\n",
    "print(\"F1:\", F1(nonstd_predictions, target_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26de3395",
   "metadata": {},
   "source": [
    "    In this case, there is no difference in the predictions made using uniform versus distance weighting. I'll utilize distance for the remainder of this assignment since it (in theory) performs better than using uniform distance in most cases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e0f13d",
   "metadata": {},
   "source": [
    "### 16. Repeat #15 a-d, but using a logistic regression with ‘elasticnet’ or ‘l2’ penalty (feel free to use sklearn.linear_model.LogisticRegression) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "eae6da5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 5197)\n",
      "accuracy: 1261.0\n",
      "F1: 0.9847715736040609\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mnek/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py:1184: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/mnek/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression()\n",
    "lr.fit(features_training, target_training)\n",
    "lr_pred = lr.predict(features_test)\n",
    "\n",
    "print(\"accuracy:\", accuracy(lr_pred, target_test))\n",
    "print(\"F1:\", F1(lr_pred, target_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40155915",
   "metadata": {},
   "source": [
    "### 17) Evaluation of an estimator performance via cross-validation: Implement the S-fold cross validation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "aa8ccf67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def sPartition(folds, data):\n",
    "    '''Returns list of ({i:training_i}, {j: test_j}) tuples in preparation for k-fold cross validation'''\n",
    "    kf = KFold(n_splits=folds)\n",
    "    partitioned_data = []\n",
    "    for _, (training, test) in enumerate(kf.split(data)):\n",
    "        partitioned_data.append(({i:data[i] for i in training}, {j:data[j] for j in test}))\n",
    "    return partitioned_data\n",
    "\n",
    "# label means real-world result of training data\n",
    "def sFold(folds, data, labels, model, model_args, error_function=mean_squared_error):\n",
    "    partitioned_data = sPartition(folds, data)\n",
    "    expected_labels_by_partition = []\n",
    "    predicted_labels_by_partition = []\n",
    "\n",
    "    for partition in partitioned_data:\n",
    "        model_ = model(**model_args)\n",
    "\n",
    "        x_ = list(partition[0].values())\n",
    "        y_ = [labels[i] for i in partition[0].keys()]\n",
    "\n",
    "        model_.fit(x_, y_)\n",
    "\n",
    "        expected = [labels[i] for i in partition[1].keys()]\n",
    "        predicted = model_.predict(list(partition[1].values()))\n",
    "\n",
    "        expected_labels_by_partition.append(expected)\n",
    "        predicted_labels_by_partition.append(predicted)\n",
    "\n",
    "    avg_error = sum([error_function(expected_labels_by_partition[i], predicted_labels_by_partition[i]) for i in range(folds)])/folds\n",
    "\n",
    "    return (expected_labels_by_partition, predicted_labels_by_partition, avg_error)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c65dd2",
   "metadata": {},
   "source": [
    "### 18) Only using the training portion of your data, use your sfold function to evaluate the performance of your model over each combination of k and distance metrics from the following sets:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8a199d",
   "metadata": {},
   "source": [
    "        i. k=[1,5,9,11]\n",
    "                b. distance = [Euclidean, Manhattan]\n",
    "        ii. weights = [uniform, distance]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "17aebf59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Experiment name   k   distance   weights  Average F1\n",
      "0    Experiment #1   1  Euclidean   uniform    0.968357\n",
      "1    Experiment #2   1  Euclidean  distance    0.968357\n",
      "2    Experiment #3   1  Manhattan   uniform    0.969282\n",
      "3    Experiment #4   1  Manhattan  distance    0.969282\n",
      "4    Experiment #5   5  Euclidean   uniform    0.979363\n",
      "5    Experiment #6   5  Euclidean  distance    0.979363\n",
      "6    Experiment #7   5  Manhattan   uniform    0.979169\n",
      "7    Experiment #8   5  Manhattan  distance    0.979169\n",
      "8    Experiment #9   9  Euclidean   uniform    0.980375\n",
      "9   Experiment #10   9  Euclidean  distance    0.980375\n",
      "10  Experiment #11   9  Manhattan   uniform    0.980371\n",
      "11  Experiment #12   9  Manhattan  distance    0.980371\n",
      "12  Experiment #13  11  Euclidean   uniform    0.980378\n",
      "13  Experiment #14  11  Euclidean  distance    0.980378\n",
      "14  Experiment #15  11  Manhattan   uniform    0.980475\n",
      "15  Experiment #16  11  Manhattan  distance    0.980475\n"
     ]
    }
   ],
   "source": [
    "df = [[\"Experiment name\", \"k\", \"distance\", \"weights\", \"Average F1\"]]\n",
    "count = 0\n",
    "for k in [1, 5, 9, 11]:\n",
    "    for distance in [\"Euclidean\", \"Manhattan\"]:\n",
    "        for weights in [\"uniform\", \"distance\"]:\n",
    "            count += 1\n",
    "            avg_error = sFold(folds=5, data=features_training, labels=target_training, model=KNN_Classifier, model_args={\"n_neighbors\": k, \"weights\": weights, \"p\": 1 if distance == \"Manhattan\" else 2}, error_function=F1)[-1]\n",
    "            df.append([\"Experiment #\"+str(count), k, distance, weights, avg_error])\n",
    "print(pd.DataFrame(df[1:], columns=df[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "8f154515",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Experiment name    Experiment #15\n",
       "k                              11\n",
       "distance                Manhattan\n",
       "weights                   uniform\n",
       "Average F1               0.980475\n",
       "Name: 14, dtype: object"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(df[1:], columns=df[0])\n",
    "df.loc[df['Average F1'].idxmax()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f4ba3c",
   "metadata": {},
   "source": [
    "    Based on the experiment conducted, the best configuration for the model would be K=11, Manhattan distance, and inverse distance weighting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "3ed49cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# recheck work on IDW vs uniform distance bc they really shouldn't be identical??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5525fda3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
